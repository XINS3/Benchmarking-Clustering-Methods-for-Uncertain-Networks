{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graph_tool.all as gt \n",
    "\n",
    "g=gt.collection.data['lesmis'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read graph txt\n",
    "\n",
    "f = open('g.txt','r')\n",
    "lines=[[n for n in x.split()] for x in f.readlines()]\n",
    "f.close()\n",
    "lines\n",
    "g = gt.Graph()\n",
    "lines_=[]\n",
    "for _ in lines:\n",
    "    l=[]\n",
    "    l.append(int(_[0]))\n",
    "    l.append(int(_[1]))\n",
    "    l.append(float(_[2]))\n",
    "    lines_.append(l)\n",
    "print(lines_)\n",
    "g.add_vertex(1 + max([l[0] for l in lines_] + [l[1] for l in lines_]))\n",
    "property_map=g.new_edge_property('float')\n",
    "for l in lines_:\n",
    "    g.add_edge(g.vertex(l[0]),g.vertex(l[1]))\n",
    "    property_map[g.edge(g.vertex(l[0]),g.vertex(l[1]))]=l[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in g.edges():\n",
    "    print(e)\n",
    "print(list(property_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#g = gt.collection.data[\"lesmis\"].copy()\n",
    "\n",
    "import random as rd \n",
    "\n",
    "#q = g.new_ep(\"double\", .98)   # edge uncertainties\n",
    "N = g.num_vertices()\n",
    "E = g.num_edges()\n",
    "#e = g.edge(11, 36)\n",
    "#q[e] = .5                     # ambiguous true edge\n",
    "q=property_map\n",
    "#e = g.add_edge(15, 73)\n",
    "#q[e] = .5                     # ambiguous spurious edge\n",
    "\n",
    "# We inititialize UncertainBlockState, assuming that each non-edge\n",
    "# has an uncertainty of q_default, chosen to preserve the expected\n",
    "# density of the original network:\n",
    "\n",
    "q_default = (E - sum(q)) / ((N * (N - 1))/2 - E)\n",
    "\n",
    "state = gt.UncertainBlockState(g, q=q, q_default=q_default)\n",
    "print('done state')\n",
    "# We will first equilibrate the Markov chain\n",
    "gt.mcmc_equilibrate(state, wait=100, mcmc_args=dict(niter=10))\n",
    "print('100')\n",
    "# Now we collect the marginals for exactly 100,000 sweeps, at\n",
    "# intervals of 10 sweeps:\n",
    "\n",
    "u = None              # marginal posterior edge probabilities\n",
    "bs = []               # partitions\n",
    "cs = []               # average local clustering coefficient\n",
    "\n",
    "def collect_marginals(s):\n",
    "   global bs, u, cs\n",
    "   print('before',s==state)\n",
    "   u = s.collect_marginal(u)\n",
    "   bstate = s.get_block_state()\n",
    "   bs.append(bstate.levels[0].b.a.copy())\n",
    "   cs.append(gt.local_clustering(s.get_graph()).fa.mean())\n",
    "   print('u',u)\n",
    "   print('bs',bs)\n",
    "print('1000')\n",
    "gt.mcmc_equilibrate(state, force_niter=1000, mcmc_args=dict(niter=10),\n",
    "                    callback=collect_marginals)\n",
    "\n",
    "eprob = u.ep.eprob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import load_graph\n",
    "import importlib\n",
    "importlib.reload(load_graph)\n",
    "import load_graph\n",
    "import Expected_mod as ex \n",
    "import clustering_bayesian_ref\n",
    "importlib.reload(clustering_bayesian_ref)\n",
    "import clustering_bayesian_ref as br \n",
    "filename='g.txt'\n",
    "g,property_map=load_graph.read_graph_tool_g(filename)\n",
    "clustering=br.clustering(g,property_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# According to Figure 11: Expected modularity according to the different community strengths\n",
    "## bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through dic choose file automatically\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import Expected_mod as ex \n",
    "import clustering_bayesian_ref\n",
    "importlib.reload(clustering_bayesian_ref)\n",
    "import clustering_bayesian_ref as br \n",
    "import load_graph\n",
    "from Expected_mod import Trans_C1\n",
    "importlib.reload(load_graph)\n",
    "import load_graph\n",
    "path='mcp_acp_data//k10_l10//'\n",
    "filelist=[i for i in os.listdir(path) if (i[-3:]=='txt' and i[10]=='1' and i[11]=='0' and i[12]=='_' )]\n",
    "filelist.sort()\n",
    "print(filelist)\n",
    "\n",
    "# run weighted louvain, save clustering\n",
    "k=10\n",
    "l=10\n",
    "value=[]\n",
    "T=[]\n",
    "clustering=[]\n",
    "rep=5\n",
    "#path='datasets//'\n",
    "for graph in filelist:\n",
    "    Emod=0\n",
    "    current_cluster=[]\n",
    "    for _ in range(rep):\n",
    "        g,property_map=load_graph.read_graph_tool_g(path+graph)\n",
    "        \n",
    "\n",
    "        edge=[]\n",
    "        p=list(property_map.a)\n",
    "        \n",
    "            \n",
    "        for e in g.edges():\n",
    "            \n",
    "            edge.append((int(e.source()),int(e.target())))\n",
    "            \n",
    "\n",
    "        t1=time.time()\n",
    "        cluster=br.clustering(g,property_map)\n",
    "        t2=time.time()\n",
    "        current_cluster.append(cluster)\n",
    "        #clustering.append(cluster)\n",
    "        Emod+=ex.APWP(edge,p,Trans_C1(cluster))\n",
    "    clustering.append(current_cluster)\n",
    "    print('----------graph: ',graph,'----------')\n",
    "    print('cluster:',cluster)\n",
    "    print('-----------Ex modularity',Emod,'-----------')\n",
    "    value.append(Emod/rep)\n",
    "    T.append(t2-t1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# run ex mod, save time and value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate AMI according to Figure 12: AMI score according to the community strengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through dic choose file automatically\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import Expected_mod as ex \n",
    "import clustering_bayesian_ref\n",
    "importlib.reload(clustering_bayesian_ref)\n",
    "import clustering_bayesian_ref as br \n",
    "import load_graph\n",
    "from Expected_mod import Trans_C1\n",
    "importlib.reload(load_graph)\n",
    "import load_graph\n",
    "from Expected_mod import Trans_C2\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score as ami\n",
    "node=100\n",
    "nomalized_cluster=[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7]\n",
    "\n",
    "path='mcp_acp_data//k10_l10//'\n",
    "filelist=[i for i in os.listdir(path) if (i[-3:]=='txt' and i[10]=='1' and i[11]=='0' and i[12]=='_' )]\n",
    "filelist.sort()\n",
    "print(filelist)\n",
    "\n",
    "# run weighted louvain, save clustering\n",
    "k=10\n",
    "l=10\n",
    "AMI=[]\n",
    "T=[]\n",
    "clustering=[]\n",
    "rep=5\n",
    "#path='datasets//'\n",
    "for graph in filelist:\n",
    "    ami_=0\n",
    "    current_cluster=[]\n",
    "    for _ in range(rep):\n",
    "        \n",
    "        g,property_map=load_graph.read_graph_tool_g(path+graph)\n",
    "        \n",
    "\n",
    "        edge=[]\n",
    "        p=list(property_map.a)\n",
    "        \n",
    "            \n",
    "        for e in g.edges():\n",
    "            \n",
    "            edge.append((int(e.source()),int(e.target())))\n",
    "            \n",
    "\n",
    "        t1=time.time()\n",
    "        cluster=br.clustering(g,property_map)\n",
    "        t2=time.time()\n",
    "        current_cluster.append(cluster)\n",
    "        #clustering.append(cluster)\n",
    "        nor_cluster=nomalized_cluster\n",
    "        c=Trans_C2(Trans_C1(cluster),node)\n",
    "        ami_+=ami(nor_cluster,c)\n",
    "    AMI.append(ami_/rep)\n",
    "\n",
    "\n",
    "    clustering.append(current_cluster)\n",
    "    print('----------graph: ',graph,'----------')\n",
    "    print('cluster:',cluster)\n",
    "    print('-----------AMI',ami_/rep,'-----------')\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# run ex mod, save time and value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AMI)\n",
    "AMI_=[]\n",
    "for _ in AMI:\n",
    "    AMI_.append(float(_))\n",
    "print(AMI_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calculate AMI according toFigure 13: AMI score according to the increasing number of clusters, community strength s = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Expected_mod import Trans_C1, APWP\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "import Expected_mod as ex \n",
    "import load_graph\n",
    "import importlib\n",
    "import time\n",
    "import clustering_bayesian_ref\n",
    "importlib.reload(clustering_bayesian_ref)\n",
    "import clustering_bayesian_ref as br \n",
    "from Expected_mod import Trans_C2\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score as ami\n",
    "\n",
    "\n",
    "importlib.reload(load_graph)\n",
    "\n",
    "path2='mcp_acp_data//l10_p0.3//datasets//'\n",
    "\n",
    "filelist1=[i for i in os.listdir(path2) if i[-3:]=='txt'  ]\n",
    "filelist1.sort()\n",
    "print(filelist1)\n",
    "\n",
    "NMI=[]\n",
    "X_=[]\n",
    "l=10\n",
    "path='mcp_acp_data//l10_p0.3//datasets//'\n",
    "for graph in filelist1:\n",
    "\n",
    "    strr=[]\n",
    "    flag=0\n",
    "   \n",
    "    for _ in graph:\n",
    "        if _=='k':\n",
    "            \n",
    "            flag=1\n",
    "            continue\n",
    "        if flag==0:\n",
    "            continue\n",
    "        if _<'0' or _>'9':\n",
    "           \n",
    "            break\n",
    "        strr.append(_)\n",
    "    \n",
    "    k=sum([int(strr[i])*(10**(len(strr)-(i+1))) for i in range(len(strr))])\n",
    "    c=[i for i in range(k*l)]\n",
    "    \n",
    "    stad_cluster=[c[x:x+l] for x in range(0, len(c), l)]\n",
    "   \n",
    "    print(graph)\n",
    "    g,property_map=load_graph.read_graph_tool_g(path+graph)\n",
    "\n",
    "    current_nmi=0\n",
    "    time=3\n",
    "    for _ in range(time):\n",
    "        cluster=br.clustering(g,property_map)\n",
    "    \n",
    "        current_nmi+=ami(Trans_C2(stad_cluster,k*l),cluster)\n",
    "    NMI.append(current_nmi/time)\n",
    "    X_.append(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment to low prob. according to Performances under Different Probability Distributions (RQ5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through dic choose file automatically\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import Expected_mod as ex \n",
    "import clustering_bayesian_ref\n",
    "importlib.reload(clustering_bayesian_ref)\n",
    "import clustering_bayesian_ref as br \n",
    "import load_graph\n",
    "from Expected_mod import Trans_C1\n",
    "importlib.reload(load_graph)\n",
    "import load_graph\n",
    "import os\n",
    "from tkinter import Tcl\n",
    "import networkx\n",
    "    \n",
    "path2='datasets//l10_p0.3_evolving_lowP1//'\n",
    "filelists=[i for i in os.listdir(path2)]\n",
    "filelists=Tcl().call('lsort','-dict',filelists)\n",
    "\n",
    "# run weighted louvain, save clustering\n",
    "\n",
    "Baye={}\n",
    "\n",
    "for file in filelists:\n",
    "    if file[-4:] =='.txt':\n",
    "        file_list=file.split('_')\n",
    "       \n",
    "        k=[i for i in file_list if i[0]=='k'][0][1:]\n",
    "        l=[i for i in file_list if i[0]=='l'][0][1:]\n",
    "\n",
    "        g=load_graph.read_g(path2+file)\n",
    "        g,property_map=load_graph.read_graph_tool_g(path2+file)\n",
    "       \n",
    "\n",
    "        edge=[]\n",
    "        p=list(property_map.a)\n",
    " \n",
    "        for e in g.edges():\n",
    "            \n",
    "            edge.append((int(e.source()),int(e.target())))\n",
    "   \n",
    "        t1=time.time()\n",
    "        cluster=br.clustering(g,property_map)\n",
    "        t2=time.time()\n",
    "        print(type(cluster))\n",
    "\n",
    "        Baye.update({k:cluster})\n",
    "\n",
    "\n",
    "\n",
    "# run ex mod, save time and value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Baye)\n",
    "baye_clustering={}\n",
    "l=10\n",
    "for i in Baye:\n",
    "   \n",
    "    k=int(i)\n",
    "    clus=Trans_C1(list(Baye[i]))\n",
    "    baye_clustering.update({i:clus})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "path='mcp_acp_data//l10_p0.3_evolving_lowP1//result//bayes//'\n",
    "with open(path+'bayes_increaseK_lowP.json','w') as fp:\n",
    "          json.dump(baye_clustering,fp,indent=4) # With indent=4 (pretty format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'bayes_increaseK_lowP.json','r') as fp:\n",
    "    data=json.load(fp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment of high prob. according to Performances under Different Probability Distribu- tions (RQ5)\n",
    "\n",
    "## have completely same graph structures with low prob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through dic choose file automatically\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import Expected_mod as ex \n",
    "import clustering_bayesian_ref\n",
    "importlib.reload(clustering_bayesian_ref)\n",
    "import clustering_bayesian_ref as br \n",
    "import load_graph\n",
    "from Expected_mod import Trans_C1\n",
    "importlib.reload(load_graph)\n",
    "import load_graph\n",
    "import os\n",
    "from tkinter import Tcl\n",
    "import networkx\n",
    "    \n",
    "path2='datasets//l10_p0.3_evolving_highP1//'\n",
    "filelists=[i for i in os.listdir(path2)]\n",
    "filelists=Tcl().call('lsort','-dict',filelists)\n",
    "\n",
    "# run weighted louvain, save clustering\n",
    "\n",
    "Baye={}\n",
    "\n",
    "for file in filelists:\n",
    "    if file[-4:] =='.txt':\n",
    "        file_list=file.split('_')\n",
    "       \n",
    "        k=[i for i in file_list if i[0]=='k'][0][1:]\n",
    "        l=[i for i in file_list if i[0]=='l'][0][1:]\n",
    "\n",
    "        g=load_graph.read_g(path2+file)\n",
    "        g,property_map=load_graph.read_graph_tool_g(path2+file)\n",
    "       \n",
    "\n",
    "        edge=[]\n",
    "        p=list(property_map.a)\n",
    " \n",
    "        for e in g.edges():\n",
    "            \n",
    "            edge.append((int(e.source()),int(e.target())))\n",
    "   \n",
    "        t1=time.time()\n",
    "        cluster=br.clustering(g,property_map)\n",
    "        t2=time.time()\n",
    "        print(type(cluster))\n",
    "\n",
    "        Baye.update({k:cluster})\n",
    "\n",
    "\n",
    "\n",
    "# run ex mod, save time and value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Baye)\n",
    "baye_clustering2={}\n",
    "l=10\n",
    "for i in Baye:\n",
    "   \n",
    "    k=int(i)\n",
    "    clus=Trans_C1(list(Baye[i]))\n",
    "    baye_clustering2.update({i:clus})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path='mcp_acp_data//l10_p0.3_evolving_highP1//result//bayes//'\n",
    "with open(path+'bayes_increaseK_highP.json','w') as fp:\n",
    "          json.dump(baye_clustering2,fp,indent=4) # Wit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'bayes_increaseK_highP.json','r') as fp:\n",
    "    data=json.load(fp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment polarized graph according to Performances under Different Probability Distributions (RQ5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through dic choose file automatically\n",
    "\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import Expected_mod as ex \n",
    "import clustering_bayesian_ref\n",
    "importlib.reload(clustering_bayesian_ref)\n",
    "import clustering_bayesian_ref as br \n",
    "import load_graph\n",
    "from Expected_mod import Trans_C1\n",
    "importlib.reload(load_graph)\n",
    "import load_graph\n",
    "import os\n",
    "from tkinter import Tcl\n",
    "import networkx\n",
    "    \n",
    "path2='datasets//l50_k2_p0.18_polarized_graph//'\n",
    "filelists=[i for i in os.listdir(path2)]\n",
    "filelists=Tcl().call('lsort','-dict',filelists)\n",
    "\n",
    "# run weighted louvain, save clustering\n",
    "\n",
    "Baye={}\n",
    "\n",
    "for file in filelists:\n",
    "    if file[-4:] =='.txt':\n",
    "        file_list=file.split('_')\n",
    "       \n",
    "        k=[i for i in file_list if i[0]=='k'][0][1:]\n",
    "        l=[i for i in file_list if i[0]=='l'][0][1:]\n",
    "        name=file_list[-2]\n",
    "        print(k)\n",
    "        print(l)\n",
    "        print(name)\n",
    "\n",
    "        g=load_graph.read_g(path2+file)\n",
    "        g,property_map=load_graph.read_graph_tool_g(path2+file)\n",
    "       \n",
    "\n",
    "        edge=[]\n",
    "        p=list(property_map.a)\n",
    " \n",
    "        for e in g.edges():\n",
    "            \n",
    "            edge.append((int(e.source()),int(e.target())))\n",
    "   \n",
    "        t1=time.time()\n",
    "        cluster=br.clustering(g,property_map)\n",
    "        t2=time.time()\n",
    "        print(file)\n",
    "\n",
    "\n",
    "        Baye.update({name:cluster})\n",
    "\n",
    "\n",
    "\n",
    "# run ex mod, save time and value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "baye_clustering3={}\n",
    "l=10\n",
    "for i in Baye:\n",
    "   \n",
    "   \n",
    "    clus=Trans_C1(list(Baye[i]))\n",
    "    baye_clustering3.update({i:clus})\n",
    "baye_clustering3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path='mcp_acp_data//l50_k2_p0.18_polarized_graph//result//bayes//'\n",
    "with open(path+'bayes_polarized.json','w') as fp:\n",
    "          json.dump(baye_clustering3,fp,indent=4) # Wit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# krogan2006-core &mips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import Expected_mod as ex \n",
    "import clustering_bayesian_ref\n",
    "importlib.reload(clustering_bayesian_ref)\n",
    "import clustering_bayesian_ref as br \n",
    "import load_graph\n",
    "from Expected_mod import Trans_C1\n",
    "importlib.reload(load_graph)\n",
    "import load_graph\n",
    "import os\n",
    "from tkinter import Tcl\n",
    "import networkx\n",
    "    \n",
    "path2='mcp_acp_data//krogan2006_core//intersec_mips//net//krogan2006_core_mips_net.txt'\n",
    "\n",
    "# run weighted louvain, save clustering\n",
    "\n",
    "Baye={}\n",
    "\n",
    "\n",
    "\n",
    "g=load_graph.read_g(path2)\n",
    "g,property_map=load_graph.read_graph_tool_g(path2)\n",
    "\n",
    "\n",
    "edge=[]\n",
    "p=list(property_map.a)\n",
    "\n",
    "for e in g.edges():\n",
    "    \n",
    "    edge.append((int(e.source()),int(e.target())))\n",
    "\n",
    "t1=time.time()\n",
    "cluster=br.clustering(g,property_map)\n",
    "t2=time.time()\n",
    "k=len(cluster)\n",
    "\n",
    "\n",
    "Baye.update({k:cluster})\n",
    "\n",
    "\n",
    "\n",
    "# run ex mod, save time and value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baye={}\n",
    "l=10\n",
    "for i in Baye:\n",
    "   \n",
    "   \n",
    "    clus=Trans_C1(list(Baye[i]))\n",
    "    baye.update({i:clus})\n",
    "baye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path='mcp_acp_data//krogan2006_core//intersec_mips//baye_results//'\n",
    "with open(path+'bayes2.json','w') as fp:\n",
    "          json.dump(baye,fp,indent=4) # Wit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# krogan2006-extended &mips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "\n",
    "import Expected_mod as ex \n",
    "import clustering_bayesian_ref\n",
    "importlib.reload(clustering_bayesian_ref)\n",
    "import clustering_bayesian_ref as br \n",
    "import load_graph\n",
    "from Expected_mod import Trans_C1\n",
    "importlib.reload(load_graph)\n",
    "import load_graph\n",
    "import os\n",
    "from tkinter import Tcl\n",
    "import networkx\n",
    "    \n",
    "path2='mcp_acp_data//krogan2006_extended//intersec_mips//krogan2006_extented_mips_net.txt'\n",
    "\n",
    "# run weighted louvain, save clustering\n",
    "\n",
    "Baye={}\n",
    "\n",
    "\n",
    "\n",
    "g=load_graph.read_g(path2)\n",
    "g,property_map=load_graph.read_graph_tool_g(path2)\n",
    "\n",
    "\n",
    "edge=[]\n",
    "p=list(property_map.a)\n",
    "\n",
    "for e in g.edges():\n",
    "    \n",
    "    edge.append((int(e.source()),int(e.target())))\n",
    "\n",
    "t1=time.time()\n",
    "cluster=br.clustering(g,property_map)\n",
    "t2=time.time()\n",
    "k=len(cluster)\n",
    "\n",
    "\n",
    "Baye.update({k:cluster})\n",
    "\n",
    "\n",
    "\n",
    "# run ex mod, save time and value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baye={}\n",
    "l=10\n",
    "for i in Baye:\n",
    "   \n",
    "   \n",
    "    clus=Trans_C1(list(Baye[i]))\n",
    "    baye.update({i:clus})\n",
    "baye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path='mcp_acp_data//krogan2006_extended//intersec_mips//baye_results//'\n",
    "with open(path+'bayes.json','w') as fp:\n",
    "          json.dump(baye,fp,indent=4) # Wit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effect of Shifting Probabilities (RQ2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Expected_mod import Trans_C1, APWP\n",
    "import os\n",
    "import importlib\n",
    "import time\n",
    "import Expected_mod as ex \n",
    "import load_graph\n",
    "import importlib\n",
    "import time\n",
    "import clustering_bayesian_ref\n",
    "importlib.reload(clustering_bayesian_ref)\n",
    "import clustering_bayesian_ref as br \n",
    "from Expected_mod import Trans_C2\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score as ami\n",
    "\n",
    "\n",
    "importlib.reload(load_graph)\n",
    "\n",
    "path2='mcp_acp_data//l10_k10_p0.4_multiplied//new//datasets'\n",
    "\n",
    "filelist1=[i for i in os.listdir(path2) if i[-3:]=='txt'  ]\n",
    "filelist1.sort()\n",
    "print(filelist1)\n",
    "\n",
    "clustering_bayes={}\n",
    "exp_mod=[]\n",
    "X_=[]\n",
    "l=10\n",
    "k=10\n",
    "multi=[0.0,0.1,0.2,0.3,0.4,0.5]\n",
    "i=0\n",
    "path='mcp_acp_data//l10_k10_p0.4_multiplied//new//datasets//'\n",
    "for graph in filelist1:\n",
    "    print(graph)\n",
    "   \n",
    "    \n",
    "    g,property_map=load_graph.read_graph_tool_g(path+graph)\n",
    "    edge=[]\n",
    "    p=list(property_map.a)\n",
    "    \n",
    "        \n",
    "    for e in g.edges():\n",
    "        \n",
    "        edge.append((int(e.source()),int(e.target())))\n",
    "\n",
    "    current_exp_mod=0\n",
    "    time=5\n",
    "    temp_cluster=[]\n",
    "    for _ in range(time):\n",
    "        cluster=br.clustering(g,property_map)\n",
    "        temp_cluster.append(cluster)\n",
    "        current_exp_mod+=ex.APWP(edge,p,Trans_C1(cluster))\n",
    "    clustering_bayes.update({multi[i]:temp_cluster})\n",
    "    i+=1\n",
    "    print(clustering_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baye={}\n",
    "l=10\n",
    "for i in clustering_bayes:\n",
    "    print(clustering_bayes[i])\n",
    "    temp=[]\n",
    "    for j in clustering_bayes[i]:\n",
    "   \n",
    "        clus=Trans_C1(list(j))\n",
    "        temp.append(clus)\n",
    "    baye.update({i:temp})\n",
    "print(baye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "path='mcp_acp_data//l10_k10_p0.4_multiplied//new//baye_results//'\n",
    "with open(path+'bayes.json','w') as fp:\n",
    "          json.dump(baye,fp,indent=4) # Wit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'bayes.json','r') as fp:\n",
    "    data=json.load(fp)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AMI - Effect of Shifting Probabilities (RQ2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score as ami\n",
    "path2='mcp_acp_data//l10_k10_p0.4_multiplied//new//datasets'\n",
    "\n",
    "filelist1=[i for i in os.listdir(path2) if i[-3:]=='txt'  ]\n",
    "filelist1.sort()\n",
    "print(filelist1)\n",
    "\n",
    "exp_mod=[]\n",
    "\n",
    "l=10\n",
    "k=10\n",
    "multi=[1,10,2,4,6,8]\n",
    "i=0\n",
    "cc=[i for i in range(k*l)]\n",
    "    \n",
    "stad_cluster=[cc[x:x+l] for x in range(0, len(cc), l)]\n",
    "path='mcp_acp_data//l10_k10_p0.4_multiplied//new//datasets//'\n",
    "for graph in filelist1:\n",
    "    print(graph)\n",
    "    strr=[]\n",
    "    flag=0\n",
    "   \n",
    "    for _ in graph:\n",
    "        if _=='i':\n",
    "            \n",
    "            flag=1\n",
    "            continue\n",
    "        if flag==0:\n",
    "            continue\n",
    "        if _<'0' or _>'9':\n",
    "           \n",
    "            break\n",
    "        strr.append(_)\n",
    "    \n",
    "    multi=multi=str(0)+'.'+str(strr[1])#sum([int(strr[i])*(10**(len(strr)-(i+1))) for i in range(len(strr))])\n",
    "\n",
    "    g,property_map=load_graph.read_graph_tool_g(path+graph)\n",
    "    edge=[]\n",
    "    p=list(property_map.a)\n",
    "    for e in g.edges():\n",
    "        \n",
    "        edge.append((int(e.source()),int(e.target())))\n",
    "\n",
    "    temp_mod=0\n",
    "   \n",
    "    for c in data[str(multi)]:\n",
    "        rep=len(data[str(multi)])\n",
    "        clu=Trans_C1(c)\n",
    "        print(c)\n",
    "        temp_mod+=ami(Trans_C2(stad_cluster,k*l),Trans_C2(c,k*l))\n",
    "\n",
    "    exp_mod.append(temp_mod/rep)\n",
    "    print(multi,':',exp_mod)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected value -Effect of Shifting Probabilities (RQ2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.cluster import adjusted_mutual_info_score as ami\n",
    "from Expected_mod import APWP\n",
    "path2='mcp_acp_data//l10_k10_p0.4_multiplied//new//datasets'\n",
    "\n",
    "filelist1=[i for i in os.listdir(path2) if i[-3:]=='txt'  ]\n",
    "filelist1.sort()\n",
    "print(filelist1)\n",
    "\n",
    "exp_mod=[]\n",
    "\n",
    "l=10\n",
    "k=10\n",
    "\n",
    "i=0\n",
    "cc=[i for i in range(k*l)]\n",
    "    \n",
    "stad_cluster=[cc[x:x+l] for x in range(0, len(cc), l)]\n",
    "path='mcp_acp_data//l10_k10_p0.4_multiplied//new//datasets//'\n",
    "for graph in filelist1:\n",
    "    print(graph)\n",
    "    strr=[]\n",
    "    flag=0\n",
    "   \n",
    "    for _ in graph:\n",
    "        if _=='i':\n",
    "            \n",
    "            flag=1\n",
    "            continue\n",
    "        if flag==0:\n",
    "            continue\n",
    "        if _<'0' or _>'9':\n",
    "           \n",
    "            break\n",
    "        strr.append(_)\n",
    "    \n",
    "    multi=multi=str(0)+'.'+str(strr[1])#sum([int(strr[i])*(10**(len(strr)-(i+1))) for i in range(len(strr))])\n",
    "\n",
    "    g,property_map=load_graph.read_graph_tool_g(path+graph)\n",
    "    edge=[]\n",
    "    p=list(property_map.a)\n",
    "    for e in g.edges():\n",
    "        \n",
    "        edge.append((int(e.source()),int(e.target())))\n",
    "\n",
    "    temp_mod=0\n",
    "   \n",
    "    for c in data[str(multi)]:\n",
    "        rep=len(data[str(multi)])\n",
    "        clu=Trans_C1(c)\n",
    "        print(c)\n",
    "        temp_mod+=APWP(edge,p,clu)\n",
    "\n",
    "    exp_mod.append(temp_mod/rep)\n",
    "    print(multi,':',exp_mod)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
